{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36b06391",
   "metadata": {},
   "source": [
    "# <center>AI Guard Agent</center>\n",
    "\n",
    "## <center>EE 782 : Assignment 2</center>\n",
    "\n",
    "### <center>Naresh Kumar Meena : 22B3947</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e3aa7",
   "metadata": {},
   "source": [
    "#### Code Demo video link: \n",
    "https://iitbacin-my.sharepoint.com/:v:/g/personal/22b3947_iitb_ac_in/EUqeShecXa5FtLoyjYIOIKMBIc7RI_VA0ilie5cITwpygQ?e=7Jv1Rm&nav=eyJyZWZlcnJhbEluZm8iOnsicmVmZXJyYWxBcHAiOiJTdHJlYW1XZWJBcHAiLCJyZWZlcnJhbFZpZXciOiJTaGFyZURpYWxvZy1MaW5rIiwicmVmZXJyYWxBcHBQbGF0Zm9ybSI6IldlYiIsInJlZmVycmFsTW9kZSI6InZpZXcifX0%3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff10f3",
   "metadata": {},
   "source": [
    "#### Final Demo Video link: \n",
    "https://iitbacin-my.sharepoint.com/personal/22b3947_iitb_ac_in/_layouts/15/stream.aspx?id=%2Fpersonal%2F22b3947%5Fiitb%5Fac%5Fin%2FDocuments%2FRecording%2D20251013%5F181217%2Ewebm&referrer=StreamWebApp%2EWeb&referrerScenario=AddressBarCopied%2Eview%2E10e97d83%2Dfdee%2D4777%2D8a79%2D9e9c8789985e&isDarkMode=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52d097",
   "metadata": {},
   "source": [
    "#### Github Repository Link: \n",
    "https://github.com/nareshkmn/AIGuardAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b902c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nares\\anaconda3\\Lib\\site-packages\\face_recognition_models\\__init__.py:7: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for all system functionalities\n",
    "import os                                         # For interacting with the operating system (e.g., file deletion)\n",
    "import cv2                                        # OpenCV for camera access and image processing\n",
    "import face_recognition                           # For finding and encoding faces in images\n",
    "import numpy as np                                # For numerical operations, especially with face recognition arrays\n",
    "import pickle                                     # For loading the pre-trained face recognition model\n",
    "import time                                       # For handling delays and timing (e.g., cooldowns, pauses)\n",
    "import tempfile                                   # For creating temporary files in the system's temp directory\n",
    "import speech_recognition as sr                   # For converting speech to text (ASR)\n",
    "from gtts import gTTS                             # Google Text-to-Speech for generating audio from text\n",
    "from playsound import playsound \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pygame                                     # For reliable audio playback\n",
    "import threading                                  # For running tasks in parallel (e.g., listening for voice commands while processing video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc91b8d6",
   "metadata": {},
   "source": [
    "### Activation and Basic Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b68846",
   "metadata": {},
   "source": [
    "#### 1. Trusted User Enrollment and Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c47dbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting robust face enrollment and classifier training...\n",
      "[SUCCESS] Processed atul.png for category: atul\n",
      "[SUCCESS] Processed naresh_1.png for category: naresh\n",
      "[SUCCESS] Processed naresh_10.png for category: naresh\n",
      "[SUCCESS] Processed naresh_11.png for category: naresh\n",
      "[SUCCESS] Processed naresh_12.png for category: naresh\n",
      "[SUCCESS] Processed naresh_13.png for category: naresh\n",
      "[SUCCESS] Processed naresh_14.png for category: naresh\n",
      "[SUCCESS] Processed naresh_2.png for category: naresh\n",
      "[SUCCESS] Processed naresh_3.png for category: naresh\n",
      "[SUCCESS] Processed naresh_4.png for category: naresh\n",
      "[SUCCESS] Processed naresh_5.png for category: naresh\n",
      "[SUCCESS] Processed naresh_6.png for category: naresh\n",
      "[SUCCESS] Processed naresh_7.png for category: naresh\n",
      "[SUCCESS] Processed naresh_8.png for category: naresh\n",
      "[SUCCESS] Processed naresh_9.png for category: naresh\n",
      "[SUCCESS] Processed unknown_0.png for category: unrecognized\n",
      "[SUCCESS] Processed unknown_3.png for category: unrecognized\n",
      "[SUCCESS] Processed unknown_4.png for category: unrecognized\n",
      "[SUCCESS] Processed unknown_5.png for category: unrecognized\n",
      "\n",
      "[INFO] Found 15 trusted faces and 4 untrusted faces. Training SVM classifier...\n",
      "[INFO] Classifier training complete.\n",
      "[INFO] Saved robust trained model to known_faces_model.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "TRUSTED_FACES_DIR = \"D:/AI_Guard_Agent/trusted_faces\"\n",
    "UNTRUSTED_FACES_DIR = \"D:/AI_Guard_Agent/untrusted_faces\" # Directory for negative examples\n",
    "MODEL_FILE = \"known_faces_model.pkl\"\n",
    "\n",
    "print(\"[INFO] Starting robust face enrollment and classifier training...\")\n",
    "known_encodings = []\n",
    "known_names = []           \n",
    "\n",
    "\n",
    "# --- Data Processing Function ---           # Cite: [1]\n",
    "def process_directory(directory, name_prefix=None, is_untrusted=False):\n",
    "    \"\"\"\n",
    "    Scans a given directory for images, computes face encodings for each,\n",
    "    and appends the encodings and corresponding names to the global lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If the specified directory doesn't exist, create it.\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"[WARNING] Created '{directory}' directory. Please add images and run again.\")\n",
    "        return 0\n",
    "\n",
    "    image_count = 0\n",
    "    # Loop through every file in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                image_path = os.path.join(directory, filename)\n",
    "                \n",
    "                # If it's the untrusted directory, all faces get the same label\n",
    "                if is_untrusted:\n",
    "                    name = \"unrecognized\"\n",
    "                else:\n",
    "                    # Extracts the name from 'name_1.jpg' as 'name'\n",
    "                    name = os.path.splitext(filename)[0].split('_')[0]\n",
    "                    \n",
    "                image = face_recognition.load_image_file(image_path)      # Load the image file into a numpy array\n",
    "                face_encodings = face_recognition.face_encodings(image)   # Compute the face encoding for the first face found in the image\n",
    "                \n",
    "                # If a face was successfully found and encoded\n",
    "                if face_encodings:\n",
    "                    known_encodings.append(face_encodings[0])        # Add the encoding and name to our training data lists\n",
    "                    known_names.append(name)\n",
    "                    print(f\"[SUCCESS] Processed {filename} for category: {name}\")\n",
    "                    image_count += 1\n",
    "                else:\n",
    "                    print(f\"[WARNING] No face found in {filename}. Skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Could not process {filename}: {e}\")\n",
    "    return image_count\n",
    "\n",
    "# --- Main Execution ---\n",
    "# Process both trusted and untrusted faces\n",
    "trusted_count = process_directory(TRUSTED_FACES_DIR)\n",
    "untrusted_count = process_directory(UNTRUSTED_FACES_DIR, is_untrusted=True)\n",
    "\n",
    "\n",
    "# --- Classifier Training ---\n",
    "\n",
    "if len(np.unique(known_names)) < 2:\n",
    "    print(\"\\n[ERROR] Classifier training requires at least two different categories (e.g., one trusted person and one untrusted person).\")\n",
    "elif known_encodings:\n",
    "    print(f\"\\n[INFO] Found {trusted_count} trusted faces and {untrusted_count} untrusted faces. Training SVM classifier...\")\n",
    "    \n",
    "    # The SVM model can only work with numbers, not strings like \"name\".\n",
    "    # The LabelEncoder converts each unique name into a number (e.g., name -> 0, atul -> 1, unrecognized -> 2).\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(known_names)\n",
    "    \n",
    "    # Initialize the Support Vector Machine (SVM) classifier.\n",
    "    # 'probability=True' allows us to get confidence scores later.\n",
    "    classifier = SVC(gamma=\"scale\", probability=True)\n",
    "    classifier.fit(known_encodings, labels)              # Train the model by feeding it the face encodings (features) and the numerical labels.\n",
    "    print(\"[INFO] Classifier training complete.\")\n",
    "    \n",
    "    # Save the trained model to a file using pickle.\n",
    "    # We save both the classifier itself and the label encoder so we can decode the predictions later.\n",
    "    with open(MODEL_FILE, \"wb\") as f:\n",
    "        pickle.dump({\"classifier\": classifier, \"label_encoder\": label_encoder}, f)\n",
    "    print(f\"[INFO] Saved robust trained model to {MODEL_FILE}\")\n",
    "else:\n",
    "    print(\"[INFO] No faces were enrolled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37bae7",
   "metadata": {},
   "source": [
    "**Classifier Training (Optional Stretch Goal):** All the collected face encodings and their corresponding names are used as training data for a Support Vector Machine (SVC), a powerful machine learning model. This model learns the complex patterns that differentiate one person's face from another.\n",
    "\n",
    "Face Encoding: For each face found in the images, it uses the face_recognition library to compute a unique 128-point mathematical encoding (a vector) that represents the facial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54621a10",
   "metadata": {},
   "source": [
    "#### 2. Speech Recognition (ASR) + Text-to-Speech (TTS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3998b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] AI Guard System Initialized. Calibrating microphone...\n",
      "[INFO] Microphone calibrated. Say 'guard my room' to activate.\n",
      "[INFO] Listening for command...\n",
      "[USER SAID]: guard my room\n",
      "[GUARD SAYS]: Guard mode activated. I will protect this room.\n",
      "[INFO] Listening for command...\n",
      "[USER SAID]: stand down\n",
      "[GUARD SAYS]: Guard mode deactivated.\n",
      "[INFO] Listening for command...\n",
      "\n",
      "[INFO] Program interrupted by user. Shutting down.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "ACTIVATION_COMMAND = \"guard my room\"\n",
    "DEACTIVATION_COMMAND = \"stand down\"\n",
    "\n",
    "# This class encapsulates all the audio input (ASR) and output (TTS) functionalities.\n",
    "class AI_Guard:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the AI Guard System.\"\"\"\n",
    "        self.guard_mode_active = False       # State variable to track if the main guard mode is active or idle.\n",
    "        self.recognizer = sr.Recognizer()    # Initialize the core SpeechRecognition library components.\n",
    "        self.microphone = sr.Microphone()\n",
    "        \n",
    "        print(\"[INFO] AI Guard System Initialized. Calibrating microphone...\")\n",
    "        \n",
    "        # --- Microphone Calibration ---\n",
    "        # This is a critical step for accuracy. The system listens for 1 second of ambient\n",
    "        # background noise to learn what \"silence\" sounds like in the current environment.\n",
    "        # This helps it distinguish spoken commands from noise.\n",
    "        with self.microphone as source:                                               # Cite: [2]\n",
    "            self.recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "        print(\"[INFO] Microphone calibrated. Say 'guard my room' to activate.\")\n",
    "\n",
    "    def speak(self, text):\n",
    "        \"\"\"Converts text to speech using gTTS and plays it.\"\"\"\n",
    "        print(f\"[GUARD SAYS]: {text}\")\n",
    "        try:\n",
    "            tts = gTTS(text=text, lang='en')\n",
    "            # Use the system's temporary directory to avoid permission errors\n",
    "            temp_dir = tempfile.gettempdir()\n",
    "            audio_file = os.path.join(temp_dir, \"response.mp3\")\n",
    "            tts.save(audio_file)\n",
    "            playsound(audio_file)     # Play the saved audio file.\n",
    "            os.remove(audio_file)     # Clean up by deleting the temporary file.\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Could not speak due to an error: {e}\")\n",
    "\n",
    "    def listen_for_command(self):\n",
    "        \"\"\"Listens for a command via the microphone and transcribes it to text.\"\"\"\n",
    "        command = \"\"\n",
    "        try:\n",
    "            with self.microphone as source:\n",
    "                print(\"[INFO] Listening for command...\")\n",
    "                \n",
    "                # Listen for audio from the microphone. It will wait up to 5 seconds for speech to start\n",
    "                # and will stop listening after 4 seconds of continuous speech.\n",
    "                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=4)\n",
    "            \n",
    "            # --- Transcription ---\n",
    "            # Send the captured audio data to Google's Web Speech API for transcription.\n",
    "            # The .lower() converts the text to lowercase for easier command matching.    \n",
    "            command = self.recognizer.recognize_google(audio).lower()\n",
    "            print(f\"[USER SAID]: {command}\")\n",
    "            \n",
    "            # --- Error Handling ---\n",
    "        except sr.WaitTimeoutError:\n",
    "            pass \n",
    "        except sr.UnknownValueError:\n",
    "            print(\"[INFO] Could not understand the audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"[ERROR] Could not request results from Google Speech Recognition service; {e}\")\n",
    "        return command\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Main loop for the AI Guard with graceful shutdown.\"\"\"\n",
    "        try:\n",
    "            # This loop runs forever, continuously listening for commands.\n",
    "            while True:\n",
    "                command = self.listen_for_command()\n",
    "                \n",
    "                # --- State Management Logic ---\n",
    "                if self.guard_mode_active:\n",
    "                    # If guard mode is ON, listen for the deactivation command.\n",
    "                    if DEACTIVATION_COMMAND in command:\n",
    "                        self.guard_mode_active = False\n",
    "                        self.speak(\"Guard mode deactivated.\")\n",
    "                    else:\n",
    "                        print(\"[STATUS] Guard mode is active. Monitoring...\")\n",
    "                else:\n",
    "                    # If guard mode is OFF, listen for the activation command.\n",
    "                    if ACTIVATION_COMMAND in command:\n",
    "                        self.guard_mode_active = True\n",
    "                        self.speak(\"Guard mode activated. I will protect this room.\")\n",
    "                        \n",
    "                # Pause for a short duration to prevent the loop from using 100% CPU.\n",
    "                time.sleep(1) # Small delay to prevent high CPU usage\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n[INFO] Program interrupted by user. Shutting down.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    guard = AI_Guard()\n",
    "    guard.run()\n",
    "    \n",
    "# Cite: [11], [12], [13]   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b0671",
   "metadata": {},
   "source": [
    "Speech Recognition (ASR): The listen_for_command() method uses the SpeechRecognition library to capture audio from the microphone. The captured audio is then sent to the Google Web Speech API for transcription. During testing, this implementation proved to be highly effective, achieving over 90% accuracy in recognizing the activation and deactivation commands under normal room conditions.\n",
    "\n",
    "Text-to-Speech (TTS): The speak() method provides the agent's voice. It uses the gTTS library to convert any given text string into an .mp3 audio file, which is then played through the system's speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a3a6f",
   "metadata": {},
   "source": [
    "### Face Recognition and Verify Trusted User Enrollment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3406bfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] AI Guard System Initialized. Calibrating microphone...\n",
      "[INFO] Microphone calibrated.\n",
      "[INFO] Loaded trained face recognition model.\n",
      "\n",
      "[INFO] AI Guard is running. System is now listening for activation commands.\n",
      "[INFO] Listening for a command...\n",
      "[USER SAID]: mere record kar raha hai abhi record karega to do\n",
      "[INFO] Listening for a command...\n",
      "[USER SAID]: guard my room\n",
      "[GUARD SAYS]: Guard mode activated. Vision system online.\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[GUARD SAYS]: Welcome, naresh. Glad to see you.\n",
      "[INFO] Listening for a command...\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.76\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.78\n",
      "[GUARD SAYS]: Welcome, naresh. Glad to see you.\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.78\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.78\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.83\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.84\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.83\n",
      "[GUARD SAYS]: Welcome, naresh. Glad to see you.\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80[INFO] Listening for a command...\n",
      "\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.77\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.73\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.78\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[GUARD SAYS]: Welcome, naresh. Glad to see you.\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.78\n",
      "[USER SAID]: welcome price\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.80\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.82\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.79\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.78\n",
      "[GUARD SAYS]: Welcome, naresh. Glad to see you.\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.81[USER SAID]: stand down\n",
      "[GUARD SAYS]: Guard mode deactivated. Vision system offline.\n",
      "\n",
      "\n",
      "[INFO] Program interrupted by user. Shutting down.\n",
      "[INFO] Webcam released and all windows closed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "ACTIVATION_COMMAND = \"guard my room\"\n",
    "DEACTIVATION_COMMAND = \"stand down\"\n",
    "MODEL_FILE = \"known_faces_model.pkl\"\n",
    "\n",
    "# Confidence threshold for face recognition. A face is considered a match only if the\n",
    "# classifier's confidence is above this value. This is a key parameter to tune.\n",
    "RECOGNITION_THRESHOLD = 0.70\n",
    "\n",
    "# --- Base Class Definition ---\n",
    "# This class encapsulates all the audio input (ASR) and output (TTS) functionalities.\n",
    "class AI_Guard:\n",
    "    def __init__(self):\n",
    "        self.guard_mode_active = False\n",
    "        self.recognizer = sr.Recognizer()\n",
    "        self.microphone = sr.Microphone()\n",
    "        pygame.mixer.init()                    # Initialize the Pygame mixer for reliable audio playback\n",
    "        self.speak_lock = threading.Lock()    # Create a lock to prevent race conditions when multiple threads try to speak at once\n",
    "        print(\"[INFO] AI Guard System Initialized. Calibrating microphone...\")\n",
    "        with self.microphone as source:                                             # Cite: [2]\n",
    "            self.recognizer.adjust_for_ambient_noise(source, duration=1)        \n",
    "        print(\"[INFO] Microphone calibrated.\")\n",
    "\n",
    "    def speak(self, text):\n",
    "        \"\"\"Converts text to speech using gTTS and plays it with Pygame.\"\"\"\n",
    "        with self.speak_lock:\n",
    "            # Use the lock to ensure only one speech operation happens at a time\n",
    "            print(f\"[GUARD SAYS]: {text}\")\n",
    "            try:\n",
    "                tts = gTTS(text=text, lang='en')\n",
    "                temp_dir = tempfile.gettempdir()\n",
    "                audio_file = os.path.join(temp_dir, \"response.mp3\")\n",
    "                tts.save(audio_file)\n",
    "                \n",
    "                # Use Pygame to play the audio file\n",
    "                pygame.mixer.music.load(audio_file)\n",
    "                pygame.mixer.music.play()\n",
    "                \n",
    "                # Wait for the audio to finish playing\n",
    "                while pygame.mixer.music.get_busy():\n",
    "                    time.sleep(0.1)\n",
    "                pygame.mixer.music.unload()\n",
    "                os.remove(audio_file)    # Clean up the temporary file\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Could not speak due to an error: {e}\")\n",
    "\n",
    "    def listen_for_command(self):\n",
    "        \"\"\"Listens for a command via the microphone and transcribes it to text.\"\"\"\n",
    "        command = \"\"\n",
    "        try:\n",
    "            with self.microphone as source:\n",
    "                print(\"[INFO] Listening for a command...\")\n",
    "                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=4)\n",
    "            command = self.recognizer.recognize_google(audio).lower()\n",
    "            print(f\"[USER SAID]: {command}\")\n",
    "        except (sr.WaitTimeoutError, sr.UnknownValueError, sr.RequestError):\n",
    "            pass\n",
    "        return command\n",
    "\n",
    "# --- Vision Class ---\n",
    "# This class inherits from AI_Guard and adds all the computer vision capabilities.\n",
    "class AI_Guard_Vision(AI_Guard):\n",
    "    def __init__(self):\n",
    "        # Initialize the parent AI_Guard class (ASR, TTS)\n",
    "        super().__init__()\n",
    "        try:\n",
    "            # Load the pre-trained SVM classifier and label encoder from the pickle file\n",
    "            with open(MODEL_FILE, \"rb\") as f:\n",
    "                self.model_data = pickle.load(f)\n",
    "            print(\"[INFO] Loaded trained face recognition model.\")\n",
    "        except FileNotFoundError:\n",
    "            self.speak(f\"Error: Model file '{MODEL_FILE}' not found. Please run the enrollment script first.\")\n",
    "            exit()\n",
    "        \n",
    "        # Initialize the webcam                    # Cite: [3]\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        if not self.video_capture.isOpened():\n",
    "            self.speak(\"Error: Cannot open webcam.\")\n",
    "            exit()\n",
    "        \n",
    "        # Timestamps and state variables for managing interactions    \n",
    "        self.last_seen_trusted_time = 0\n",
    "        self.last_unrecognized_alert_time = 0\n",
    "        self.cooldown_period = 10                  # 10-second cooldown for messages\n",
    "        self.stop_event = threading.Event()       # Event to signal threads to stop\n",
    "        self.vision_window_active = False         # Flag to track if the OpenCV window is open\n",
    "\n",
    "    def process_vision(self):\n",
    "        \"\"\"The main computer vision loop: captures a frame, finds faces, and identifies them.\"\"\"\n",
    "        # Read a single frame from the webcam\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if not ret: return\n",
    "        # For performance, create a smaller version of the frame for face detection\n",
    "        rgb_small_frame = cv2.cvtColor(cv2.resize(frame, (0, 0), fx=0.25, fy=0.25), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Find all faces and their encodings in the small frame\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "        \n",
    "         # Reset state flags for the current frame\n",
    "        is_any_person_present = len(face_encodings) > 0\n",
    "        found_trusted_person = False\n",
    "        \n",
    "        # Get the trained classifier and label encoder from the loaded model\n",
    "        classifier = self.model_data[\"classifier\"]\n",
    "        label_encoder = self.model_data[\"label_encoder\"]\n",
    "        \n",
    "        # Loop through each detected face\n",
    "        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "            probabilities = classifier.predict_proba([face_encoding])[0]             # Use the trained SVM classifier to get prediction probabilities\n",
    "            best_match_index = np.argmax(probabilities)                             # Find the index of the highest probability\n",
    "            predicted_name = label_encoder.classes_[best_match_index]             # Get the name and confidence score for the best match\n",
    "            confidence = probabilities[best_match_index]\n",
    "            \n",
    "            print(f\"[DEBUG] Predicted: {predicted_name}, Confidence: {confidence:.2f}\")\n",
    "            \n",
    "            # --- Visual Feedback Logic ---       # Cite: [5]\n",
    "            # --- FEATURE IMPLEMENTATION: Set box color based on recognition ---\n",
    "            display_name = \"Unrecognized\"                                           # Cite: [6]\n",
    "            box_color = (0, 0, 255) # Red for unrecognized by default\n",
    "            # -----------------------------------------------------------------\n",
    "            \n",
    "            # Check if the confidence is high enough to be considered a match\n",
    "            if confidence > RECOGNITION_THRESHOLD:\n",
    "                found_trusted_person = True\n",
    "                display_name = predicted_name.replace('_', ' ')\n",
    "                box_color = (0, 255, 0) # Green for recognized           # Cite: [6]\n",
    "                \n",
    "                # Greet the person if enough time has passed since the last greeting\n",
    "                current_time = time.time()\n",
    "                if current_time - self.last_seen_trusted_time > self.cooldown_period:\n",
    "                    self.speak(f\"Welcome, {display_name}. Glad to see you.\")\n",
    "                    self.last_seen_trusted_time = current_time\n",
    "\n",
    "            top *= 4; right *= 4; bottom *= 4; left *= 4\n",
    "            # Use the dynamic box_color variable for drawing\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), box_color, 2)\n",
    "            label = f\"{display_name} ({confidence:.2f})\"\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), box_color, cv2.FILLED)\n",
    "            cv2.putText(frame, label, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "        #  --- Decision Logic (Post-Frame Analysis) ---         # Cite: [7]\n",
    "        # If no trusted person was found but someone is present, they are an intruder\n",
    "        if not found_trusted_person and is_any_person_present:\n",
    "            current_time = time.time()\n",
    "            if current_time - self.last_unrecognized_alert_time > self.cooldown_period:       # Issue a warning if enough time has passed since the last one\n",
    "                self.speak(\"Warning. An unrecognized person has been detected.\")\n",
    "                self.last_unrecognized_alert_time = current_time\n",
    "        \n",
    "        # Display the resulting frame in a pop-up window\n",
    "        cv2.imshow('AI Guard Vision', frame)\n",
    "        self.vision_window_active = True\n",
    "\n",
    "    def _threaded_listener(self):        # Cite: [8]\n",
    "        \"\"\"This function runs in a separate thread, dedicated to listening for voice commands.\"\"\"\n",
    "        while not self.stop_event.is_set():\n",
    "            command = self.listen_for_command()\n",
    "            # Process commands to activate or deactivate the guard\n",
    "            if ACTIVATION_COMMAND in command and not self.guard_mode_active:\n",
    "                self.guard_mode_active = True\n",
    "                self.speak(\"Guard mode activated. Vision system online.\")\n",
    "            elif DEACTIVATION_COMMAND in command and self.guard_mode_active:\n",
    "                self.guard_mode_active = False\n",
    "                self.speak(\"Guard mode deactivated. Vision system offline.\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"The main application entry point.\"\"\"\n",
    "        self.stop_event.clear()\n",
    "        # Create and start the background listener thread\n",
    "        listener_thread = threading.Thread(target=self._threaded_listener, daemon=True)\n",
    "        listener_thread.start()\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n[INFO] AI Guard is running. System is now listening for activation commands.\")\n",
    "             # This main loop handles vision processing and window management\n",
    "            while not self.stop_event.is_set():\n",
    "                if self.guard_mode_active:\n",
    "                    self.process_vision()   # If active, process the webcam feed\n",
    "                else:\n",
    "                    # If idle, ensure the vision window is closed\n",
    "                    if self.vision_window_active:\n",
    "                        cv2.destroyWindow('AI Guard Vision')\n",
    "                        self.vision_window_active = False\n",
    "\n",
    "                # Check if the 'q' key was pressed in the OpenCV window to quit\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    print(\"\\n[INFO] 'q' key pressed. Shutting down.\")\n",
    "                    break\n",
    "                \n",
    "                time.sleep(0.05 if self.guard_mode_active else 0.5)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n[INFO] Program interrupted by user. Shutting down.\")\n",
    "        finally:\n",
    "             # --- Graceful Shutdown ---\n",
    "            self.stop_event.set()\n",
    "            listener_thread.join(timeout=1.0)\n",
    "            self.video_capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            pygame.mixer.quit()\n",
    "            print(\"[INFO] Webcam released and all windows closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    guard = AI_Guard_Vision()\n",
    "    guard.run()\n",
    "\n",
    "# Cite: [11], [12], [13], [14] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18736978",
   "metadata": {},
   "source": [
    "### Escalation Dialogue and Full System Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb846ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Groq client configured successfully.\n",
      "[INFO] AI Guard System Initialized. Calibrating microphone...\n",
      "[INFO] Microphone calibrated.\n",
      "[INFO] Loaded trained face recognition model.\n",
      "\n",
      "[INFO] AI Guard is running. Say 'guard my room' to activate.\n",
      "[INFO] Listening for a command...\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[USER SAID]: guard my room\n",
      "[GUARD SAYS]: Guard mode activated.\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.75\n",
      "[GUARD SAYS]: Welcome back, naresh. Glad to see you.\n",
      "[INFO] Listening for a command...\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[USER SAID]: the cardboard is activated\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.75\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.62\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.69\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.73\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.71\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.73\n",
      "[GUARD SAYS]: Welcome back, naresh. Glad to see you.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.74\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.65\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.68\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.69\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.71\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.69\n",
      "[GUARD SAYS]: Welcome back, naresh. Glad to see you.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[USER SAID]: set reminder\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.71\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.67\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.59\n",
      "[USER SAID]: the minimum threshold value of 17\n",
      "[GUARD SAYS]: Could you please tell me who you are and how you got access to this room?\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.55\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.65\n",
      "[GUARD SAYS]: Welcome back, naresh. Glad to see you.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[USER SAID]: so now let's\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.61\n",
      "[GUARD SAYS]: Hi, could you please tell me who you are and how you got here?\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.[DEBUG] Predicted: unrecognized, Confidence: 0.58\n",
      "\n",
      "[INFO] Listening for a command...[DEBUG] Predicted: unrecognized, Confidence: 0.66\n",
      "\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.68\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.68\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.64\n",
      "[ALERT] Escalating to level 2.\n",
      "[GUARD SAYS]: Excuse me, but this is a private hostel room and I need it to myself.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.\n",
      "[INFO] Listening for a command...\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.\n",
      "[INFO] Listening for a command...\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.\n",
      "[INFO] Listening for a command...[DEBUG] Predicted: unrecognized, Confidence: 0.62\n",
      "\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.66\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.65\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.63\n",
      "[ALERT] Escalating to level 3.\n",
      "[GUARD SAYS]: You are trespassing and will be reported to the hostel warden immediately if you do not vacate the premises.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.63\n",
      "[ALERT] Escalating to level 4.\n",
      "[ALARM] Intruder has not left. Sounding alarm.\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.65\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.68\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.64\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.66\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.65\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.62\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.59\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.64\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.68\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.63\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.67\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.67\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.70\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.65\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.59[INFO] Google Speech Recognition could not understand audio.\n",
      "\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: unrecognized, Confidence: 0.61\n",
      "[GUARD SAYS]: Hi, can I help you with something - do you have a reservation for a room here in the hostel?\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[USER SAID]: 100% appear\n",
      "[INFO] Listening for a command...\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[USER SAID]: twisted\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.76\n",
      "[GUARD SAYS]: Welcome back, naresh. Glad to see you.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "[INFO] Google Speech Recognition could not understand audio.\n",
      "[INFO] Listening for a command...\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.67\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.75\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.70\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.65\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.73\n",
      "[DEBUG] Predicted: naresh, Confidence: 0.73\n",
      "[GUARD SAYS]: Welcome back, naresh. Glad to see you.\n",
      "[INFO] Transcribing with Google Speech Recognition...\n",
      "\n",
      "[INFO] Program interrupted by user.\n",
      "[INFO] System resources released.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Google Speech Recognition could not understand audio.\n"
     ]
    }
   ],
   "source": [
    "# --- System Configuration ---\n",
    "\n",
    "ACTIVATION_COMMAND = \"guard my room\"\n",
    "DEACTIVATION_COMMAND = \"stand down\"\n",
    "MODEL_FILE = \"known_faces_model.pkl\"\n",
    "\n",
    "# Confidence threshold for face recognition. A face is considered a match only if the\n",
    "# classifier's confidence is above this value. This is a key parameter to tune.\n",
    "RECOGNITION_THRESHOLD = 0.70\n",
    "\n",
    "# --- PASTE YOUR GROQ API KEY HERE ---             # Cite : [4], [10]\n",
    "GROQ_API_KEY = \"YOUR GROQ API KEY\"\n",
    "\n",
    "# --- Global Client Initialization for the LLM ---\n",
    "\n",
    "# Initialize the 'client' variable to None. It will be configured if an API key is provided.\n",
    "client = None\n",
    "try:\n",
    "    # Check if a valid API key has been provided\n",
    "    if GROQ_API_KEY != \"YOUR_GROQ_API_KEY\" and GROQ_API_KEY:\n",
    "        from groq import Groq  # Import the Groq library only if needed\n",
    "        # Create the client object to communicate with the Groq API\n",
    "        client = Groq(api_key=GROQ_API_KEY)\n",
    "        print(\"[INFO] Groq client configured successfully.\")\n",
    "    else:\n",
    "        print(\"[WARNING] Groq API Key is not set. LLM features will be disabled.\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to configure Groq client: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- Base AI Guard Class ---\n",
    "\n",
    "# This class handles the core audio input (ASR) and output (TTS) functionalities.\n",
    "class AI_Guard:\n",
    "    def __init__(self):\n",
    "        self.guard_mode_active = False         # State variable to track if the guard mode is active\n",
    "        self.recognizer = sr.Recognizer()      # Initialize the speech recognizer\n",
    "        self.microphone = sr.Microphone()      # Initialize the microphone\n",
    "        pygame.mixer.init()                    # Initialize the Pygame mixer for reliable audio playback\n",
    "        \n",
    "        self.speak_lock = threading.Lock()     # Create a threading lock to prevent multiple parts of the program from trying to speak at the exact same time, which can cause file access errors.\n",
    "\n",
    "        print(\"[INFO] AI Guard System Initialized. Calibrating microphone...\")\n",
    "        \n",
    "        # Listen for 1 second to adjust the recognizer for ambient noise levels              # Cite: [2]\n",
    "        with self.microphone as source:\n",
    "            self.recognizer.adjust_for_ambient_noise(source, duration=1)         \n",
    "        print(\"[INFO] Microphone calibrated.\")\n",
    "        \n",
    "    def speak(self, text, is_alarm=False):\n",
    "        with self.speak_lock:\n",
    "            # If an alarm is already playing, don't interrupt it with speech\n",
    "            if pygame.mixer.music.get_busy() and is_alarm is False:\n",
    "                return\n",
    "\n",
    "            print(f\"[GUARD SAYS]: {text}\")\n",
    "            try:\n",
    "                tts = gTTS(text=text, lang='en')                       # Create the gTTS object with the text to be spoken\n",
    "                temp_dir = tempfile.gettempdir()                       # Get the system's temporary directory path to avoid permission errors\n",
    "                audio_file = os.path.join(temp_dir, \"response.mp3\")    # Define the full path for the temporary audio file\n",
    "                tts.save(audio_file)                                   # Save the generated speech to the mp3 file\n",
    "\n",
    "                pygame.mixer.music.load(audio_file)        # Use Pygame's music mixer to play the audio file\n",
    "                pygame.mixer.music.play()\n",
    "            \n",
    "                # Wait in a loop until the audio has finished playing\n",
    "                while pygame.mixer.music.get_busy():\n",
    "                    time.sleep(0.1)\n",
    "                pygame.mixer.music.unload()       # Unload the file so it can be safely deleted\n",
    "                os.remove(audio_file)             # Remove the temporary audio file\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Could not speak due to an error: {e}\")\n",
    "        # The lock is automatically released here\n",
    "                \n",
    "\n",
    "\n",
    "    def listen_for_command(self):\n",
    "        \"\"\"Listens for a command via the microphone and uses Google Web Speech API.\"\"\"\n",
    "        command = \"\"\n",
    "        try:\n",
    "            with self.microphone as source:\n",
    "                print(\"[INFO] Listening for a command...\")\n",
    "                audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=4)     # Listen for up to 5 seconds, stopping after 4 seconds of speech\n",
    "            \n",
    "            print(\"[INFO] Transcribing with Google Speech Recognition...\")       # Use Google's online service to convert the audio to text\n",
    "            command = self.recognizer.recognize_google(audio).lower()\n",
    "            print(f\"[USER SAID]: {command}\")\n",
    "\n",
    "        # Handle common exceptions for speech recognition\n",
    "        except sr.WaitTimeoutError:\n",
    "            pass # This is expected if no one speaks\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"[INFO] Google Speech Recognition could not understand audio.\")\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"[ERROR] Could not request results from Google service; {e}\")\n",
    "        return command\n",
    "\n",
    "\n",
    "# --- Full System Class ---\n",
    "\n",
    "# This class inherits from AI_Guard and adds vision, LLM, and state management.\n",
    "class AI_Guard_Full(AI_Guard):\n",
    "    def __init__(self):\n",
    "        super().__init__()          # Initialize the parent AI_Guard class (ASR, TTS, etc.)\n",
    "        try:\n",
    "            # Load the pre-trained SVM classifier and label encoder from the pickle file\n",
    "            with open(MODEL_FILE, \"rb\") as f:\n",
    "                self.model_data = pickle.load(f)\n",
    "            print(\"[INFO] Loaded trained face recognition model.\")\n",
    "        except FileNotFoundError:\n",
    "            self.speak(f\"Error: Model file '{MODEL_FILE}' not found. Please run enroll_faces.py first.\")\n",
    "            exit()\n",
    "        \n",
    "        # Initialize the webcam                           # Cite: [3]\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        if not self.video_capture.isOpened():\n",
    "            self.speak(\"Error: Cannot open webcam.\")\n",
    "            exit()\n",
    "        \n",
    "        # Timestamps to prevent spamming welcome/warning messages\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        self.last_seen_trusted_time = 0\n",
    "        self.cooldown_period = 10  # 10-second cooldown\n",
    "        # Event to signal the background listener thread to stop\n",
    "        self.stop_event = threading.Event()\n",
    "        # Flag to track if the OpenCV window is currently open\n",
    "        self.vision_window_active = False\n",
    "        # Dictionary to manage the state of an intruder encounter\n",
    "        self.intruder_state = {\"detected\": False, \"start_time\": None, \"escalation_level\": 0, \"last_warning_time\": 0}\n",
    "        # Time intervals (in seconds) for escalating warnings\n",
    "        # --- NEW: Added Level 4 for the alarm ---\n",
    "        self.escalation_intervals = {1: 0, 2: 10, 3: 20, 4: 30} # seconds\n",
    "        self.alarm_sound = self.generate_alarm_sound()\n",
    "        self.is_alarm_playing = False\n",
    "    \n",
    "    # Cite : [15]\n",
    "    # --- Alarm Sound Generation ---    \n",
    "    def generate_alarm_sound(self, beep_duration=0.15, silence_duration=0.1, num_beeps=3, frequency=2000):\n",
    "        \"\"\"Generates a rapid, high-pitched beeping sound, like a fire alarm.\"\"\"\n",
    "        sample_rate = pygame.mixer.get_init()[0]\n",
    "        \n",
    "        # Calculate the number of samples for one beep and the silence that follows\n",
    "        beep_samples = int(sample_rate * beep_duration)\n",
    "        silence_samples = int(sample_rate * silence_duration)\n",
    "        \n",
    "        # Generate the high-pitched beep tone\n",
    "        beep_wave = (np.sin(2 * np.pi * np.arange(beep_samples) * frequency / sample_rate)).astype(np.float32)\n",
    "        # Create a silent segment\n",
    "        silence_wave = np.zeros(silence_samples, dtype=np.float32)\n",
    "        \n",
    "        # Combine one beep and one silence period\n",
    "        single_alarm_cycle = np.concatenate([beep_wave, silence_wave])\n",
    "        \n",
    "        # Repeat the cycle to create a series of beeps\n",
    "        alarm_wave = np.tile(single_alarm_cycle, num_beeps)\n",
    "        \n",
    "        # Convert to 16-bit PCM format and make it stereo\n",
    "        alarm_wave = (alarm_wave * 32767).astype(np.int16)\n",
    "        sound_buffer = np.repeat(alarm_wave.reshape(-1, 1), 2, axis=1)\n",
    "        \n",
    "        # Return the final sound object that Pygame can play\n",
    "        return pygame.sndarray.make_sound(sound_buffer)\n",
    "\n",
    "\n",
    "\n",
    "    def generate_response(self, level):\n",
    "        \"\"\"Generates a spoken response from the LLM based on the escalation level.\"\"\"\n",
    "        if not client: return \"Language model not available.\"\n",
    "        \n",
    "        # Context-specific prompts for a college hostel room environment\n",
    "        system_prompts = {\n",
    "            1: \"You are a friendly AI assistant guarding a college hostel room. In one short, casual sentence, politely ask who they are.\",\n",
    "            2: \"The unrecognized person has not left. In one short sentence, state that this is a private hostel room and they need to leave.\",\n",
    "            3: \"The intruder is still here. In one short, stern sentence, state that they are trespassing and the hostel warden will be alerted if they don't leave.\"\n",
    "        }\n",
    "        \n",
    "        # Get the appropriate prompt for the current level\n",
    "        prompt_text = system_prompts.get(level, \"An error occurred.\")\n",
    "        try:\n",
    "            # Send the prompt to the Groq API using the Llama 3.1 model\n",
    "            chat_completion = client.chat.completions.create(messages=[{\"role\": \"system\", \"content\": prompt_text}], model=\"llama-3.1-8b-instant\")\n",
    "            # Extract and return the generated text\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return \"My response circuits are offline.\"\n",
    "\n",
    "\n",
    "    def handle_unrecognized_person(self):\n",
    "        \"\"\"Manages the state and logic for an escalating encounter with an intruder.\"\"\"\n",
    "        current_time = time.time()\n",
    "        # If this is the first time seeing an intruder\n",
    "        if not self.intruder_state[\"detected\"]:\n",
    "            # Update the state to start the encounter\n",
    "            self.intruder_state.update({\"detected\": True, \"start_time\": current_time, \"escalation_level\": 1})\n",
    "            # Generate and speak the Level 1 warning\n",
    "            response = self.generate_response(1)\n",
    "            self.speak(response)\n",
    "        else:\n",
    "            # If an intruder is already detected, check if it's time to escalate\n",
    "            time_since_detection = current_time - self.intruder_state[\"start_time\"]\n",
    "            new_level = 0\n",
    "            # --- Check for Level 4 escalation ---\n",
    "            if time_since_detection > self.escalation_intervals[4] and self.intruder_state[\"escalation_level\"] < 4: new_level = 4\n",
    "            elif time_since_detection > self.escalation_intervals[3] and self.intruder_state[\"escalation_level\"] < 3: new_level = 3\n",
    "            elif time_since_detection > self.escalation_intervals[2] and self.intruder_state[\"escalation_level\"] < 2: new_level = 2\n",
    "            \n",
    "            if new_level > self.intruder_state[\"escalation_level\"]:\n",
    "                self.intruder_state[\"escalation_level\"] = new_level\n",
    "                print(f\"[ALERT] Escalating to level {new_level}.\")\n",
    "                \n",
    "                if new_level == 4:\n",
    "                    # --- NEW: Play the alarm sound ---\n",
    "                    print(\"[ALARM] Intruder has not left. Sounding alarm.\")\n",
    "                    if not self.is_alarm_playing:\n",
    "                        self.alarm_sound.play(loops=-1) # Play indefinitely\n",
    "                        self.is_alarm_playing = True\n",
    "                else:\n",
    "                    response = self.generate_response(new_level)\n",
    "                    self.speak(response)\n",
    "\n",
    "    def reset_intruder_state(self):\n",
    "        \"\"\"Resets the intruder encounter state back to default.\"\"\"\n",
    "        if self.intruder_state[\"detected\"]:\n",
    "            self.intruder_state = {\"detected\": False, \"start_time\": None, \"escalation_level\": 0}\n",
    "            # --- Alarm Feature: Stop the alarm when the threat is cleared ---\n",
    "            if self.is_alarm_playing:\n",
    "                self.alarm_sound.stop()\n",
    "                self.is_alarm_playing = False\n",
    "\n",
    "\n",
    "    def process_vision(self):\n",
    "        \"\"\"The main computer vision loop: captures frame, finds faces, and identifies them.\"\"\"\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if not ret: return\n",
    "        \n",
    "        rgb_small_frame = cv2.cvtColor(cv2.resize(frame, (0, 0), fx=0.25, fy=0.25), cv2.COLOR_BGR2RGB)     # Create a smaller version of the frame for faster face recognition\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)                                 # Find all faces and their encodings in the small frame\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "        \n",
    "        # Flags to track the state of the current frame\n",
    "        is_trusted_person_present_in_frame = False\n",
    "        is_any_person_present_in_frame = len(face_encodings) > 0\n",
    "\n",
    "        # Get the trained classifier and label encoder from the loaded model\n",
    "        classifier = self.model_data[\"classifier\"]\n",
    "        label_encoder = self.model_data[\"label_encoder\"]\n",
    "\n",
    "        # Loop through each detected face\n",
    "        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "            probabilities = classifier.predict_proba([face_encoding])[0]       # Use the trained SVM classifier to get prediction probabilities for each known category\n",
    "            best_match_index = np.argmax(probabilities)                        # Find the category with the highest probability\n",
    "            predicted_name = label_encoder.classes_[best_match_index]\n",
    "            confidence = probabilities[best_match_index]\n",
    "            \n",
    "            print(f\"[DEBUG] Predicted: {predicted_name}, Confidence: {confidence:.2f}\")\n",
    "\n",
    "\n",
    "             # Cite : [6]\n",
    "            # A person is trusted IF AND ONLY IF the prediction is NOT \"unrecognized\" AND the confidence is high.\n",
    "            if predicted_name != \"unrecognized\" and confidence > RECOGNITION_THRESHOLD:\n",
    "                is_trusted_person_present_in_frame = True\n",
    "                display_name = predicted_name.replace('_', ' ')\n",
    "                box_color = (0, 255, 0) # Green for trusted\n",
    "\n",
    "                current_time = time.time()\n",
    "                # Greet the trusted person if enough time has passed since the last greeting\n",
    "                if current_time - self.last_seen_trusted_time > self.cooldown_period:\n",
    "                    self.speak(f\"Welcome back, {display_name}. Glad to see you.\")\n",
    "                    self.last_seen_trusted_time = current_time\n",
    "            else:\n",
    "                # If the prediction IS \"unrecognized\" OR the confidence for a known person is too low, they are treated as an intruder.\n",
    "                display_name = \"Unrecognized\"\n",
    "                box_color = (0, 0, 255) # Red for untrusted\n",
    "            \n",
    "            # --- Visual Feedback on the Pop-up Window ---\n",
    "            top *= 4; right *= 4; bottom *= 4; left *= 4                                            # Scale face locations back up\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), box_color, 2)                        # Draw the box\n",
    "            label = f\"{display_name} ({confidence:.2f})\"                                            # Create the text label\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), box_color, cv2.FILLED)       # Draw label background\n",
    "            cv2.putText(frame, label, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)   # Draw label text\n",
    "\n",
    "        # Cite: [7]\n",
    "        # --- Final Decision Logic (Post-Frame Analysis) ---\n",
    "        if is_trusted_person_present_in_frame:\n",
    "            # If at least one trusted person is in the frame, the room is secure.\n",
    "            self.reset_intruder_state()\n",
    "        elif is_any_person_present_in_frame:\n",
    "            # If people are present, but NONE were trusted, they are intruders.\n",
    "            self.handle_unrecognized_person()\n",
    "        else: # No people in the frame\n",
    "            # If the room is empty, any ongoing alert can be reset.\n",
    "            self.reset_intruder_state()\n",
    "        \n",
    "        # Display the resulting frame in a pop-up window\n",
    "        cv2.imshow('AI Guard System', frame)\n",
    "        self.vision_window_active = True\n",
    "\n",
    "    \n",
    "    # Cite: [8]\n",
    "    def _threaded_listener(self):\n",
    "        \"\"\"This function runs in a separate thread, dedicated to listening for voice commands.\"\"\"\n",
    "        while not self.stop_event.is_set():\n",
    "            command = self.listen_for_command()\n",
    "            # Process the command to activate or deactivate the guard\n",
    "            if ACTIVATION_COMMAND in command and not self.guard_mode_active:\n",
    "                self.guard_mode_active = True; self.speak(\"Guard mode activated.\")\n",
    "            elif DEACTIVATION_COMMAND in command and self.guard_mode_active:\n",
    "                self.guard_mode_active = False; self.speak(\"Guard mode deactivated.\"); self.reset_intruder_state()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"The main application entry point.\"\"\"\n",
    "        self.stop_event.clear()\n",
    "        # Create and start the background listener thread\n",
    "        listener_thread = threading.Thread(target=self._threaded_listener, daemon=True)\n",
    "        listener_thread.start()\n",
    "        try:\n",
    "            print(f\"\\n[INFO] AI Guard is running. Say '{ACTIVATION_COMMAND}' to activate.\")\n",
    "            # The main loop now primarily handles vision processing and window management\n",
    "            while not self.stop_event.is_set():\n",
    "                if self.guard_mode_active:\n",
    "                    self.process_vision()\n",
    "                else:\n",
    "                    # If idle, ensure the vision window is closed\n",
    "                    if self.vision_window_active:\n",
    "                        cv2.destroyWindow('AI Guard System')\n",
    "                        self.vision_window_active = False\n",
    "                \n",
    "                # Check if the 'q' key was pressed in the OpenCV window to quit\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "                \n",
    "                # Sleep to manage CPU usage. Short sleep when active, longer when idle.\n",
    "                time.sleep(0.05 if self.guard_mode_active else 0.5)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n[INFO] Program interrupted by user.\")\n",
    "        finally:\n",
    "            # --- Graceful Shutdown ---\n",
    "            self.stop_event.set()               # Signal all threads to stop\n",
    "            listener_thread.join(timeout=1.0)   # Wait for the listener thread to finish\n",
    "            self.video_capture.release()        # Release the webcam\n",
    "            cv2.destroyAllWindows()             # Close all OpenCV windows\n",
    "            pygame.mixer.quit()                 # Quit the Pygame mixer\n",
    "            print(\"[INFO] System resources released.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block runs when the script is executed directly\n",
    "    guard = AI_Guard_Full()\n",
    "    guard.run()\n",
    "\n",
    "\n",
    "# Cite: [11], [12], [13], [14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d55f340",
   "metadata": {},
   "source": [
    "#### How it works:\n",
    "To Activate the AI Guard Mode: Say - \"Guard My Roomm\"\n",
    "\n",
    "TO Deactivate Say - \"Stand Down\"\n",
    "\n",
    "When an recognize person appears in front of the camera it detects them as recongnized person showing a green flag box with high confidence score when threshold value or confidence score above 0.70, I set threshold value 0.70. THe AI Guard Agents greets them as Welcome back, name, Glad to see you.\n",
    "\n",
    "When an unrecognized or intruder person appears, the system flags them with a red box and initiates a level 1 warning using the Groq API.  When the intuder still appears in front of the webcam, it escalates through level 2... and then level 3.\n",
    "\n",
    " When the final verbal warning is ignored... the system enters level four, activating an audible alarm to alert neighbors.\"  \n",
    "\n",
    " This alarm continues until an untrusted person disappears or a trusted person, like myself, is seen, or until the system is deactivated. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b71cb",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "Also Implemented Optional stretch goal:\n",
    "\n",
    "This demonstration has shown our AI Guard's ability to activate by voice with above 90% accuracy, robustly differentiate between trusted and untrusted individuals or face recognition above 80% accuracy, and handle intruders with an intelligent LLM-powered conversational agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96058cfe",
   "metadata": {},
   "source": [
    "## <center>Thank You! </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7070154",
   "metadata": {},
   "source": [
    "### Following references taken for this assignment:\n",
    "\n",
    "[1] - Gemini prompt : How can I write a script that scans a folder of images, computes face encodings using face_recognition, and saves them to a .pkl file?\n",
    "\n",
    "[2] - Gemini prompt : How can I make the speech recognition more robust by calibrating the microphone for ambient noise using recognizer.adjust_for_ambient_noise?\n",
    "\n",
    "[3] - Gemini prompt : How do I use a webcam with OpenCV to compare detected faces against the saved encodings from my .pkl file?\n",
    "\n",
    "[4] - Gemini prompt : How do I use the Groq API in Python to generate escalating spoken warnings for an intruder based on the time they have been present?  \n",
    "\n",
    "[5] - Gemini prompt : How do I show the live webcam feed in a pop-up window using OpenCV (cv2.imshow) and draw bounding boxes on detected faces?\n",
    "\n",
    "[6] - Gemini prompt : How can I make the OpenCV window draw a green box for recognized faces and a red box for unrecognized ones?\n",
    "\n",
    "[7] - Gemini prompt : Why does my code say \"Welcome Unrecognized\"? How do I fix the logic to only welcome trusted people and always treat \"unrecognized\" predictions as intruders?\n",
    "\n",
    "[8] - Gemini prompt : My application freezes while listening for commands. How can I use Python's threading module to run the voice listener in the background so the video feed remains smooth?\n",
    "\n",
    "[9] - Gemini prompt : How do I add a try...finally block to my main loop to ensure that the webcam (video_capture.release()) and all\n",
    "\n",
    "[10] - Google prompt : https://console.groq.com/docs/quickstart\n",
    "\n",
    "[11] - Google prompt : https://console.groq.com/docs/text-chat\n",
    "\n",
    "[12] - Google prompt : https://console.groq.com/docs/speech-to-text\n",
    "\n",
    "[13] - Google prompt : https://console.groq.com/docs/text-to-speech\n",
    "\n",
    "[14] - Google prompt: https://console.groq.com/docs/vision \n",
    "\n",
    "[15] - Gemini prompt: How to make a audible sound alarm, trigger when intruder ingnores the final warning?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
