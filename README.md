# AI Guard Agent
An intelligent, voice-activated guard for a hostel room security, featuring robust face recognition and an escalating, LLM-powered conversational agent.

## Overview

This project is an advanced AI Guard Agent designed to monitor a college hostel room using a standard laptop's webcam and microphone. The agent activates on a voice command, uses a robust machine learning model to distinguish between trusted individuals and strangers, and engages with unrecognized persons through a series of escalating, context-aware spoken warnings generated by a Large Language Model (LLM).
The system is built with a high-performance, multi-threaded architecture to ensure a responsive and seamless user experience.

### Key Features
* **Vocal Activation:** The agent is activated and deactivated using simple voice commands ("guard my room" / "stand down").

* **Robust Face Recognition:** Utilizes a Support Vector Machine (SVM) classifier trained on multiple images per person for high-accuracy recognition, even under varying conditions.

* **Real-time Visual Feedback:** A pop-up camera window displays the live feed, drawing bounding boxes around detected faces and showing the predicted identity along with the model's confidence score.

* **Context-Aware Intruder Dialogue:** When an unrecognized person is detected, the agent initiates an escalating conversation with prompts specifically designed for a hostel environment, powered by the Groq LLM with Llama 3.1.

* **High-Performance Design:** The application is multi-threaded, separating the continuous voice command listener from the main vision processing loop. This ensures the vision system starts instantly upon activation and remains highly responsive.

* **Polished User Experience:** Includes graceful shutdown procedures, clear status messages, and robust error handling for audio and hardware.

## System Architecture

The agent operates on a state-based logic flow, integrating multiple AI modalities:

* **Speech Recognition (ASR):** A background thread continuously listens for activation or deactivation commands.

* **State Management:** Upon receiving a valid command, the system's state (guard_mode_active) is changed.

* **Computer Vision:** When active, the main thread processes the webcam feed. It detects faces and uses the pre-trained SVM classifier to identify individuals.

* **LLM Interaction:** If an unrecognized person is detected, the system generates a context-specific prompt and sends it to the Groq LLM.

## File Structure:
* AI_guard_mode.ipynb: The main Jupyter Notebook containing all the code for enrollment, modular tests, and the final integrated application.
* requirements.txt: A list of all necessary Python libraries for easy installation.
* trusted_faces/: A directory where you must place the training images for trusted individuals.


## Setup and Installation
Follow these steps to set up and run the project.

### 1. Prerequisites
Python 3.8+

Jupyter Notebook or JupyterLab installed.

pip (Python package installer)

### 2. Clone the Repository
Clone this repository to your local machine:

git clone <your-repo-url>

cd <your-repo-folder>

### 3. Set Up a Virtual Environment (Highly Recommended)
This isolates the project's dependencies from your system's Python installation.

### 4. Install Dependencies
Install all the required libraries using the requirements.txt file.

 **pip install -r requirements.txt**
 
# You also need to register your virtual environment with Jupyter
python -m ipykernel install --user --name=ai-guard-env 
'''




## How to Run the Application
### Step 1: Configure the API Key
Before running, you must add your secret Groq API key.
 
* Open the AI_guard_mode.ipynb file.
* Find the line GROQ_API_KEY = "YOUR_GROQ_API_KEY"
* Replace "YOUR_GROQ_API_KEY" with the key you obtained from the Groq Console.

### Step 2: Enroll Trusted Faces
The system needs to learn who to trust.

* Create a folder named trusted_faces in the main project directory.
* Add 3-5 varied photos of each trusted person into this folder.
* Important: Name the files using the format name_number.jpg (e.g., name_1.jpg, name_2.png, name2_1.jpg). The script uses the part of the filename before the underscore as the person's name.
* Run the enrollment script from your terminal:
* python enroll_faces.py
* This will train the SVM model and create a known_faces_model.pkl file. Re-run this script whenever you add or change photos



